from typing import List, Dict, Any, Optional
import logging
import json
import re
from .llm_interface import LLMInterface
import os
from .llm_adapter import get_llm_adapter
from emergency_fix import fix_json, extract_data_from_text

logger = logging.getLogger(__name__)

class InterviewGenerator:
    """Handles generation of interview scripts, questions, and evaluations."""
    
    def __init__(self, llm_interface: LLMInterface):
        """Initialize with an LLM interface for text generation."""
        self.llm = llm_interface
        logger.info("Initialized InterviewGenerator")
        
    def generate_interview_script(self, job_data: Dict[str, Any], 
                                  company_data: Dict[str, Any], 
                                  candidate_data: Dict[str, Any], 
                                  demo_mode: bool = False) -> Dict[str, Any]:
        """Generate an interview script using the LLM."""
        logger.info("Generating interview script")
        
        try:
            # Create prompt for the LLM
            prompt = self._create_script_generation_prompt(job_data, company_data, candidate_data, demo_mode)
            
            # Generate script using the LLM
            try:
                response = self.llm.generate_text(prompt)
                logger.info("Successfully received response from LLM")
                
                # Try to parse the response into a structured script
                try:
                    script = self._parse_script_response(response)
                    logger.info("Successfully parsed interview script")
                    return script
                except ValueError as parse_error:
                    logger.error(f"Error parsing script response: {str(parse_error)}")
                    raise
            except Exception as llm_error:
                logger.error(f"Error generating text from LLM: {str(llm_error)}")
                raise
        except Exception as e:
            logger.error(f"Error generating interview script: {str(e)}")
            logger.exception("Exception details:")
            
            # Generate a fallback script
            logger.info("Generating fallback interview script")
            return self._generate_fallback_script(job_data, demo_mode)
    
    def _create_script_generation_prompt(self, job_data: Dict[str, Any], 
                                        company_data: Dict[str, Any], 
                                        candidate_data: Dict[str, Any],
                                        demo_mode: bool = False) -> str:
        """Create a prompt for generating the interview script."""
        
        job_title = job_data.get('title', 'the position')
        company_name = company_data.get('name', 'our company')
        
        # Set question counts based on demo mode
        job_specific_count = 2 if demo_mode else 5
        technical_count = 1 if demo_mode else "3-5"
        company_fit_count = 1 if demo_mode else "2-3"
        behavioral_count = 1 if demo_mode else "3-4"
        
        prompt = f"""
        You are an AI interviewer for {company_name}. You need to conduct an interview for the {job_title} position.
        
        Job Description:
        {job_data.get('description', 'Not provided')}
        
        Required Skills:
        {job_data.get('required_skills', 'Not provided')}
        
        Company Information:
        {company_data.get('description', 'Not provided')}
        Company Values: {company_data.get('values', 'Not provided')}
        
        Candidate Information:
        Name: {candidate_data.get('name', 'the candidate')}
        Experience: {candidate_data.get('experience', 'Not provided')}
        Background: {candidate_data.get('background', 'Not provided')}
        
        Create a professional interview script with:
        1. A personalized introduction welcoming the candidate
        2. A set of interview questions categorized as:
           - Job-specific questions ({job_specific_count} questions)
           - Technical questions ({technical_count} questions)
           - Company fit questions ({company_fit_count} questions)
           - Behavioral questions ({behavioral_count} questions)
        3. A professional closing statement
        
        For each question, include:
        - The question text
        - The purpose of asking this question
        - What to look for in a good answer
        
        IMPORTANT: Use a diverse range of question formats to make the interview feel natural and varied. Include a mix of:
        - Direct knowledge questions: "What is your understanding of...?"
        - Skill assessment questions: "How would you implement...?"
        - Hypothetical scenarios: "Imagine you're faced with [scenario], what would your approach be?"
        - Opinion-based questions: "What do you think about...?"
        - Experience-sharing questions: "Tell me about a time when..." (use this format sparingly)
        - Problem-solving questions: "How would you solve...?"
        - Self-assessment questions: "What would you consider your biggest strength in...?"
        
        Avoid falling into repetitive patterns like starting every question with "Can you share an example of..." or "Could you tell me about a time when...". Make each question distinct in its structure and wording.
        
        Format the response as JSON with the following structure:
        ```json
        {{
            "introduction": "...",
            "questions": {{
                "job_specific": [
                    {{"question": "...", "purpose": "...", "good_answer_criteria": "..."}}
                ],
                "technical": [
                    {{"question": "...", "purpose": "...", "good_answer_criteria": "..."}}
                ],
                "company_fit": [
                    {{"question": "...", "purpose": "...", "good_answer_criteria": "..."}}
                ],
                "behavioral": [
                    {{"question": "...", "purpose": "...", "good_answer_criteria": "..."}}
                ]
            }},
            "closing": "..."
        }}
        ```
        """
        return prompt
    
    def _parse_script_response(self, response: str) -> Dict[str, Any]:
        """Parse the LLM response into a structured interview script."""
        logger.info("Successfully received response from LLM")
        logger.debug(f"Raw LLM response length: {len(response)} chars")
        
        try:
            # First, look for JSON in the response
            import json
            
            # Check for content in code blocks
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                logger.debug("Found JSON in code block")
                json_str = json_match.group(1)
            else:
                # Check if the entire response looks like it might be JSON
                json_str = response.strip()
                if not (json_str.startswith('{') and json_str.endswith('}')):
                    logger.warning("No JSON structure found in response, using full text")
            
            # Pre-process to fix common JSON issues
            logger.info("Attempting to fix potential JSON errors")
            json_str = self._fix_json_string(json_str)
            
            # Try to parse the JSON
            script = json.loads(json_str)
            logger.info("Successfully parsed JSON")
            
            # If we get here, we have valid JSON.
            # Check for required fields and add them if missing
            required_fields = ['introduction', 'questions', 'closing']
            for field in required_fields:
                if field not in script:
                    logger.warning(f"Missing required field '{field}' in script")
                    if field == 'introduction':
                        script['introduction'] = "Welcome to your interview. I'm excited to learn more about your background and experience."
                    elif field == 'questions':
                        script['questions'] = {}
                    elif field == 'closing':
                        script['closing'] = "Thank you for your time today. We'll be in touch regarding next steps."
            
            # Check for required question categories
            question_categories = ['job_specific', 'technical', 'company_fit', 'behavioral']
            if 'questions' in script:
                for category in question_categories:
                    if category not in script['questions']:
                        logger.warning(f"Missing question category '{category}' in script")
                        
                        # Add at least one question per missing category
                        if category == 'job_specific':
                            script['questions'][category] = [{
                                "question": "Could you tell me about your relevant experience for this position?",
                                "purpose": "To understand the candidate's background",
                                "good_answer_criteria": "Specific examples that demonstrate required skills"
                            }]
                        elif category == 'technical':
                            script['questions'][category] = [{
                                "question": "Can you describe a technical challenge you faced recently and how you resolved it?",
                                "purpose": "To assess problem-solving abilities",
                                "good_answer_criteria": "Clear problem description and effective solution"
                            }]
                        elif category == 'company_fit':
                            script['questions'][category] = [{
                                "question": "What interests you most about working with our company?",
                                "purpose": "To gauge cultural fit",
                                "good_answer_criteria": "Alignment with company values"
                            }]
                        elif category == 'behavioral':
                            script['questions'][category] = [{
                                "question": "Tell me about a time when you had to adapt to a significant change.",
                                "purpose": "To assess adaptability",
                                "good_answer_criteria": "Positive attitude toward change, specific actions taken"
                            }]
                        else:
                            script['questions'][category] = []
            
            logger.info("Successfully parsed interview script")
            return script
            
        except Exception as e:
            logger.error(f"Error parsing script response: {str(e)}")
            logger.warning("Falling back to default interview script")
            
            # Generate a fallback script since parsing failed
            return self._generate_fallback_script(self.job_data, self.demo_mode)
    
    def _fix_json_string(self, json_str: str) -> str:
        """Fix common JSON issues."""
        try:
            import json
            
            # Log the error to help with debugging
            try:
                json.loads(json_str)
                return json_str  # If we can parse it already, just return it
            except json.JSONDecodeError as e:
                logger.debug(f"JSON error detected: {str(e)}")
            
            # Apply fixes (these might help in some cases)
            # 1. Replace single quotes with double quotes
            json_str = json_str.replace("'", '"')
            
            # 2. Fix missing quotes around property names
            import re
            json_str = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_str)
            
            # 3. Fix trailing commas
            json_str = re.sub(r',\s*}', '}', json_str)
            json_str = re.sub(r',\s*]', ']', json_str)
            
            # 4. Add missing closing braces/brackets
            open_braces = json_str.count('{')
            close_braces = json_str.count('}')
            if open_braces > close_braces:
                json_str += '}' * (open_braces - close_braces)
            
            open_brackets = json_str.count('[')
            close_brackets = json_str.count(']')
            if open_brackets > close_brackets:
                json_str += ']' * (open_brackets - close_brackets)
            
            # Check if our fixes worked
            try:
                json.loads(json_str)
                return json_str
            except json.JSONDecodeError as e:
                logger.debug(f"Error after initial fixes: {str(e)}")
                raise  # Re-raise the exception to be caught by the caller
                
        except Exception as e:
            logger.error(f"Failed to fix JSON string: {str(e)}")
            raise  # Make sure to re-raise the exception so the caller knows it failed
        
        return json_str  # This should never be reached, but just in case
    
    def _aggressively_fix_json(self, json_str: str) -> str:
        """More aggressive JSON fixing for severely malformed responses."""
        import re
        import json
        
        # First try our standard fixes
        json_str = self._fix_json_string(json_str)
        
        # Try to identify the specific error location
        try:
            json.loads(json_str)
            return json_str  # If no error, return as is
        except json.JSONDecodeError as e:
            error_line = e.lineno
            error_col = e.colno
            error_msg = str(e)
            logger.warning(f"JSON error at line {error_line}, column {error_col}: {error_msg}")
            
            # Extract all lines
            lines = json_str.split('\n')
            
            # Handle the line 2 column 5 issue which is very common
            if error_line == 2 and error_col == 5:
                logger.debug("Targeted fix for line 2 column 5 error")
                if len(lines) >= 2:
                    # Try simply quoting what looks like a property name
                    match = re.match(r'(\s*)([a-zA-Z_][a-zA-Z0-9_]*)(\s*:)(.*)', lines[1])
                    if match:
                        indent, prop_name, colon, rest = match.groups()
                        lines[1] = f'{indent}"{prop_name}"{colon}{rest}'
                    json_str = '\n'.join(lines)
            
            # If the error is "Expecting ',' delimiter", try to insert a comma
            elif "Expecting ',' delimiter" in error_msg and error_line <= len(lines):
                problematic_line = lines[error_line - 1]
                # Insert a comma at the error position
                fixed_line = problematic_line[:error_col] + ',' + problematic_line[error_col:]
                lines[error_line - 1] = fixed_line
                json_str = '\n'.join(lines)
            
            # If the error is "Expecting property name", try to fix the format
            elif "Expecting property name" in error_msg and error_line <= len(lines):
                problematic_line = lines[error_line - 1]
                # Look for an unquoted property name pattern
                match = re.search(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)(\s*:)', problematic_line)
                if match:
                    prefix, prop_name, suffix = match.groups()
                    fixed_line = problematic_line.replace(f"{prefix}{prop_name}{suffix}", f'{prefix}"{prop_name}"{suffix}')
                    lines[error_line - 1] = fixed_line
                else:
                    # If we can't find a property pattern, use a generic fix
                    fixed_line = problematic_line[:error_col] + '"missing_property":' + problematic_line[error_col:]
                    lines[error_line - 1] = fixed_line
                
                json_str = '\n'.join(lines)
        
        # Try one last general approach - quote all property-like patterns
        try:
            json.loads(json_str)
            return json_str
        except:
            # Quote all patterns that look like property names
            json_str = re.sub(r'(?<=[{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r' "\1":', json_str)
            
            try:
                # See if it's valid now
                json.loads(json_str)
                return json_str
            except:
                # One last approach: try a complete structural rebuild
                if json_str.strip().startswith('{') and json_str.strip().endswith('}'):
                    # Try extracting key sections we can identify
                    intro_match = re.search(r'"introduction"\s*:\s*"([^"\\]*(\\.[^"\\]*)*)"\s*,?', json_str)
                    questions_start = json_str.find('"questions"')
                    closing_match = re.search(r'"closing"\s*:\s*"([^"\\]*(\\.[^"\\]*)*)"\s*}?', json_str)
                    
                    if intro_match and closing_match:
                        intro_text = intro_match.group(1)
                        closing_text = closing_match.group(1)
                        
                        # Create a minimal valid structure
                        min_json = (
                            '{\n'
                            f'    "introduction": "{intro_text}",\n'
                            '    "questions": {\n'
                            '        "job_specific": [],\n'
                            '        "technical": [],\n'
                            '        "company_fit": [],\n'
                            '        "behavioral": []\n'
                            '    },\n'
                            f'    "closing": "{closing_text}"\n'
                            '}'
                        )
                        
                        try:
                            json.loads(min_json)
                            logger.warning("Created minimal valid JSON from extracted sections")
                            return min_json
                        except:
                            pass
        
        # If all else fails
        logger.error("All JSON fixing attempts failed, falling back to minimal valid structure")
        return '{}'
    
    def _generate_fallback_script(self, job_data: Dict[str, Any], demo_mode: bool = False) -> Dict[str, Any]:
        """Generate a basic fallback interview script if the LLM fails."""
        logger.info("Generating fallback interview script")
        
        job_title = job_data.get('title', 'the position')
        
        # Basic fallback script
        script = {
            "introduction": f"Hello and welcome to the interview for {job_title}. Thank you for taking the time to speak with us today.",
            "questions": {
                "job_specific": [
                    {
                        "question": f"Could you tell me about your experience related to {job_title}?",
                        "purpose": "To understand the candidate's relevant experience",
                        "good_answer_criteria": "Specific examples of relevant work"
                    }
                ],
                "technical": [
                    {
                        "question": "What technical skills do you bring to this role?",
                        "purpose": "To assess technical capabilities",
                        "good_answer_criteria": "Relevant technical skills with examples"
                    }
                ],
                "company_fit": [
                    {
                        "question": "What do you know about our company?",
                        "purpose": "To assess company research",
                        "good_answer_criteria": "Knowledge of company and its values"
                    }
                ],
                "behavioral": [
                    {
                        "question": "Tell me about a challenging situation you faced at work and how you handled it.",
                        "purpose": "To assess problem-solving abilities",
                        "good_answer_criteria": "Clear problem description, actions taken, and results"
                    }
                ]
            },
            "closing": "Thank you for your time today. We'll be in touch regarding next steps."
        }
        
        # If in demo mode, don't add the second job-specific question
        if not demo_mode:
            script["questions"]["job_specific"].append({
                "question": "What interests you most about this position?",
                "purpose": "To gauge the candidate's motivation",
                "good_answer_criteria": "Alignment with job responsibilities"
            })
        
        return script
    
    def generate_follow_up(self, question: str, response: str) -> Optional[str]:
        """Generate a follow-up question based on the candidate's response."""
        logger.info("Generating follow-up question")
        
        # Track follow-up history to avoid repetitive follow-ups
        if not hasattr(self, '_follow_up_count'):
            self._follow_up_count = {}
            
        # Get the count for this question or initialize to 0
        question_key = question[:50]  # Use prefix as key to avoid memory issues
        current_count = self._follow_up_count.get(question_key, 0)
        
        # Limit follow-ups per question to prevent getting stuck
        # Allow up to 2 follow-ups for more dynamic conversation
        if current_count >= 2:
            logger.info(f"Already asked multiple follow-ups for this question, moving on")
            return None
            
        # If response is too short, follow up but don't go overboard
        if len(response.split()) < 20:
            self._follow_up_count[question_key] = current_count + 1
            return "Could you please elaborate more on your answer? I'd like to hear more specific details about your experience."
        
        # Create enhanced prompt for follow-up generation with better context awareness 
        # and advanced conversation capabilities
        prompt = f"""
        You are an expert technical interviewer with excellent conversational skills. You need to decide if and how to follow up on a candidate's response.

        CONVERSATION CONTEXT:
        Original question: "{question}"
        Candidate's response: "{response}"

        ANALYSIS INSTRUCTIONS:
        1. Evaluate the response quality on these dimensions:
           - Completeness: Does it fully address all aspects of the question?
           - Specificity: Does it include concrete examples, metrics, or details that demonstrate actual experience?
           - Technical depth: Does it show appropriate technical understanding for the role?
           - Relevance: Does it directly address the core intent of the question?
           - Communication: Is it clear, structured, and professionally articulated?
        
        2. Identify what's missing or could be expanded upon:
           - What specific examples could strengthen their response?
           - What technical details would you like them to elaborate on?
           - What aspects of the question did they overlook or not fully address?
           - What follow-up would reveal more about their capabilities or experience?

        RESPONSE INSTRUCTIONS:
        If the response is truly comprehensive and no valuable follow-up is needed, respond with EXACTLY: "NO_FOLLOW_UP_NEEDED"
        
        Otherwise, craft ONE follow-up question that:
        - Targets the most important gap or opportunity in their response
        - Uses natural, conversational language (like a real interviewer would)
        - Encourages specific examples or technical details
        - Shows active listening by referencing something they said
        - Is open-ended (not yes/no)
        - Feels like a natural progression of the conversation

        Your entire response must be ONLY the follow-up question text with no additional commentary.
        Make the question direct, concise, and specific.
        """
        
        try:
            follow_up = self.llm.generate_text(prompt)
            
            # Strictly clean the response
            cleaned_follow_up = self._extract_question_only(follow_up)
            
            # Check for explicit rejection
            if "NO_FOLLOW_UP_NEEDED" in follow_up:
                logger.info("No follow-up needed (explicit rejection)")
                return None
                
            # Check if the response indicates no follow-up is needed
            if cleaned_follow_up.strip().lower() in ["none", "no follow-up needed", "no follow-up necessary", ""]:
                logger.info("No follow-up needed")
                return None
            
            # Increment the follow-up count for this question
            self._follow_up_count[question_key] = current_count + 1
            
            logger.info("Generated follow-up question")
            return cleaned_follow_up.strip()
        except Exception as e:
            logger.error(f"Error generating follow-up question: {str(e)}")
            return None
            
    def _extract_question_only(self, text: str) -> str:
        """Extract just the question from a potentially verbose LLM response."""
        import re
        
        # Remove any explanatory phrases that might precede the question
        text = re.sub(r'(?i)(follow-up question:|i would ask:|here\'s a follow-up:|my follow-up would be:|follow up:|answer:)', '', text)
        
        # Check for a question mark - the most reliable indicator of a question
        question_sentences = re.findall(r'[^.!?]*\?', text)
        if question_sentences:
            # Return the first question found
            return question_sentences[0].strip()
            
        # Try splitting by newlines and take the last non-empty line that isn't a standard rejection
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        filtered_lines = [line for line in lines if line.lower() not in ["none", "no follow-up needed", "no follow-up necessary"]]
        
        if filtered_lines:
            # Get the last non-rejection line
            return filtered_lines[-1].strip()
            
        # If all else fails, return a cleaned version of the original text
        cleaned = re.sub(r'\s+', ' ', text).strip()
        
        # If it's a very long response, it's likely reasoning rather than a question
        if len(cleaned) > 150:
            return ""
            
        return cleaned
    
    def generate_interview_summary(
            self, 
            job_data: Dict[str, Any], 
            company_data: Dict[str, Any],
            candidate_data: Dict[str, Any],
            responses: List[Dict[str, Any]],
            early_termination: bool = False,
            follow_ups: Optional[List[Dict[str, Any]]] = None
        ) -> Dict[str, Any]:
        """Generate a summary of the interview based on the candidate's responses."""
        logger.info("Generating interview summary")
        
        # Log inputs for debugging
        logger.debug(f"Summary generation inputs: {len(responses)} responses, early_termination={early_termination}")
        
        # Set follow_ups as an instance attribute so it's available to _create_summary_prompt
        if follow_ups is not None:
            self.follow_ups = follow_ups
        else:
            self.follow_ups = []
        
        try:
            # Prepare a prompt for the LLM to generate a structured summary
            prompt = self._create_summary_prompt(job_data, company_data, candidate_data, responses, early_termination)
            
            # Generate the summary using the LLM
            response = self.llm.generate_text(prompt)
            
            # Parse the response
            summary = self._parse_summary_response(response, candidate_data, job_data)
            
            logger.info("Successfully generated interview summary")
            logger.debug(f"Summary generation result keys: {list(summary.keys())}")
            
            return summary
            
        except Exception as e:
            logger.error(f"Error generating interview summary: {str(e)}")
            logger.exception("Exception details:")
            
            # Create a basic summary with candidate info
            return self._create_fallback_summary(candidate_data, job_data)
    
    def _create_summary_prompt(self, job_data: Dict[str, Any], 
                               company_data: Dict[str, Any], 
                               candidate_data: Dict[str, Any],
                               responses: List[Dict[str, Any]],
                               early_termination: bool = False) -> str:
        """Create a clearer prompt for generating the interview summary."""
        job_title = job_data.get('title', 'the position')
        company_name = company_data.get('name', 'the company')
        candidate_name = candidate_data.get('name', 'the candidate')
        
        prompt = f"""
        Generate a comprehensive interview summary for {candidate_name} who interviewed for the {job_title} position at {company_name}.

        Return ONLY a valid JSON object with this exact format:
        {{
            "candidate_name": "{candidate_name}",
            "position": "{job_title}",
            "strengths": [
                "Specific strength 1 with evidence",
                "Specific strength 2 with evidence",
                "Specific strength 3 with evidence"
            ],
            "areas_for_improvement": [
                "Specific area 1 with suggestion",
                "Specific area 2 with suggestion"
            ],
            "technical_evaluation": "Detailed assessment of technical skills...",
            "cultural_fit": "Evaluation of candidate's alignment with company values...",
            "recommendation": "Clear hiring recommendation with justification...",
            "next_steps": "Specific actionable next steps in the hiring process...",
            "overall_assessment": "Balanced evaluation with key insights..."
        }}

        CRITICAL INSTRUCTIONS:
        - Return ONLY the JSON object with no additional text
        - All property names must be in double quotes
        - All string values must be in double quotes
        - Arrays must have comma after each item EXCEPT the last one
        - Your JSON MUST be syntactically valid
        """
        
        # Add the interview questions and responses
        prompt += "\n\nBased on these interview responses:\n\n"
        
        for i, response in enumerate(responses):
            q_data = response.get('question', {})
            q_text = q_data.get('question', f'Question {i+1}') if isinstance(q_data, dict) else str(q_data)
            r_text = response.get('response', 'No response provided')
            category = q_data.get('category', 'general') if isinstance(q_data, dict) else 'general'
            
            prompt += f"Question {i+1} [{category}]: {q_text}\n"
            prompt += f"Response: {r_text}\n\n"
        
        return prompt
    
    def _parse_summary_response(self, response: str, candidate_data: Dict[str, Any], job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse the LLM response using our comprehensive fix."""
        try:
            # Try to extract and parse the JSON using our emergency fix module
            logger.debug(f"Raw summary response length: {len(response)} chars")
            
            # Use the emergency fix module
            summary = fix_json(response)
            logger.info("Successfully parsed summary data")
            
            # Validate and return
            return self._validate_summary(summary, candidate_data, job_data)
        except Exception as e:
            logger.error(f"All parsing methods failed: {str(e)}")
            logger.debug(f"Error details: {type(e).__name__}: {str(e)}")
            
            try:
                # Last attempt - direct extraction from text
                extracted = extract_data_from_text(response)
                logger.info("Used direct text extraction for summary")
                return self._validate_summary(extracted, candidate_data, job_data)
            except Exception as extraction_error:
                logger.error(f"Even extraction failed: {str(extraction_error)}")
                return self._create_fallback_summary(candidate_data, job_data)
    
    def _validate_summary(self, summary: Dict[str, Any], candidate_data: Dict[str, Any], job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure all required fields are present with strong defaults."""
        # Define robust defaults based on context
        defaults = {
            "candidate_name": candidate_data.get("name", "Candidate"),
            "position": job_data.get("title", "Position"),
            "strengths": [
                "Participated in the interview process",
                "Demonstrated interest in the position",
                "Communicated responses to interview questions"
            ],
            "areas_for_improvement": [
                "Could provide more detailed examples in responses",
                "Additional technical assessment recommended"
            ],
            "technical_evaluation": f"The candidate discussed relevant experience for the {job_data.get('title', 'position')} role. Further technical assessment would provide a more complete evaluation.",
            "cultural_fit": f"Based on interview responses, the candidate showed interest in the company. Additional discussion would help determine alignment with company values.",
            "recommendation": "Consider for an additional technical assessment to better evaluate skills and fit.",
            "next_steps": f"Schedule a technical skills assessment. Follow up on specific areas mentioned during the interview.",
            "overall_assessment": f"{candidate_data.get('name', 'The candidate')} completed the interview process for the {job_data.get('title', 'position')} role. Their responses provided initial insights, and a follow-up technical assessment would allow for a more comprehensive evaluation."
        }
        
        # Ensure all fields exist with meaningful content
        for field, default in defaults.items():
            if field not in summary or not summary[field]:
                logger.warning(f"Missing or empty field '{field}' in summary - using default")
                summary[field] = default
            elif isinstance(default, list) and not isinstance(summary[field], list):
                logger.warning(f"Field '{field}' should be a list but isn't - fixing")
                if isinstance(summary[field], str):
                    # Try to convert string to list
                    items = [item.strip() for item in summary[field].split(',') if item.strip()]
                    summary[field] = items if items else default
                else:
                    summary[field] = default
        
        # Ensure lists have at least one item
        for field in ["strengths", "areas_for_improvement"]:
            if not summary[field] or len(summary[field]) == 0:
                logger.warning(f"Empty list for '{field}' - adding default items")
                summary[field] = defaults[field]
        
        # Critical fix: Ensure response_scores has the correct structure
        # The frontend expects each item to have a 'question_index', 'score', and 'feedback' field
        if 'response_scores' not in summary or not summary['response_scores'] or not isinstance(summary['response_scores'], list):
            logger.warning("Creating default response_scores with proper structure")
            # Create a default response scores array with at least one item containing the expected fields
            summary['response_scores'] = [
                {
                    "question_index": 0,
                    "score": 3,  # Default middle score (assuming 1-5 scale)
                    "feedback": "Response demonstrated basic understanding of the question."
                }
            ]
        else:
            # Ensure each item in response_scores has the required fields
            fixed_scores = []
            for idx, score_item in enumerate(summary['response_scores']):
                if not isinstance(score_item, dict):
                    # If item is not a dict, create a properly structured one
                    fixed_score = {
                        "question_index": idx,
                        "score": 3,
                        "feedback": "Response demonstrated basic understanding of the question."
                    }
                else:
                    # Ensure the existing dict has all required fields
                    fixed_score = score_item.copy()
                    if 'question_index' not in fixed_score:
                        fixed_score['question_index'] = idx
                    if 'score' not in fixed_score or not isinstance(fixed_score['score'], (int, float)):
                        fixed_score['score'] = 3
                    if 'feedback' not in fixed_score or not fixed_score['feedback']:
                        fixed_score['feedback'] = "Response demonstrated basic understanding of the question."
                
                fixed_scores.append(fixed_score)
            
            summary['response_scores'] = fixed_scores
        
        # Ensure skill_ratings exists and has valid structure
        if 'skill_ratings' not in summary or not isinstance(summary['skill_ratings'], list) or len(summary['skill_ratings']) == 0:
            logger.warning("Missing or invalid skill_ratings - adding defaults")
            summary['skill_ratings'] = [
                {"name": "Technical Knowledge", "score": 65},
                {"name": "Communication", "score": 70},
                {"name": "Problem Solving", "score": 60},
                {"name": "Domain Experience", "score": 55},
                {"name": "Team Collaboration", "score": 75}
            ]
        else:
            # Validate each skill rating
            fixed_ratings = []
            for i, rating in enumerate(summary['skill_ratings']):
                if not isinstance(rating, dict) or 'name' not in rating or 'score' not in rating:
                    logger.warning(f"Invalid skill rating at index {i} - fixing")
                    fixed_ratings.append({"name": f"Skill {i+1}", "score": 60})
                else:
                    # Copy and fix if needed
                    fixed_rating = rating.copy()
                    if not isinstance(fixed_rating['score'], (int, float)):
                        logger.warning(f"Invalid score for skill {fixed_rating.get('name', f'Skill {i+1}')} - fixing")
                        try:
                            fixed_rating['score'] = int(fixed_rating['score'])
                        except (ValueError, TypeError):
                            fixed_rating['score'] = 60
                    fixed_ratings.append(fixed_rating)
            
            summary['skill_ratings'] = fixed_ratings
        
        # Log the validation results
        logger.info(f"Validated summary with {len(summary.get('strengths', []))} strengths and {len(summary.get('areas_for_improvement', []))} improvement areas")
        
        return summary
    
    def _fix_advanced_json_issues(self, json_str: str) -> str:
        """Fix common JSON issues with more advanced regex patterns."""
        try:
            # Replace single quotes with double quotes for properties
            json_str = re.sub(r"(\s*)('.*?')(\s*:)", r'\1"\2"\3', json_str)
            
            # Fix missing colons in key-value pairs (exact line 2 column 36 issue)
            json_str = re.sub(r'("[\w\s]+")(\s+)(?!:)("[\w\s]+")', r'\1: \3', json_str)
            
            # Fix missing quotes around property names
            json_str = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_\s]*)(\s*:)', r'\1"\2"\3', json_str)
            
            # Fix trailing commas in objects and arrays
            json_str = re.sub(r',(\s*[\]}])', r'\1', json_str)
            
            # Fix extra commas after last array items
            json_str = re.sub(r',(\s*])', r'\1', json_str)
            
            # Fix line breaks within string literals
            json_str = re.sub(r'(".*?)(\r?\n)(.*?")', r'\1\\n\3', json_str)
            
            # Fix missing quotes around string values
            json_str = re.sub(r': *([a-zA-Z][a-zA-Z0-9_\s]*[a-zA-Z0-9])(\s*[,}])', r': "\1"\2', json_str)
            
            # Fix missing closing brackets
            open_curly = json_str.count('{')
            close_curly = json_str.count('}')
            if open_curly > close_curly:
                json_str += "}" * (open_curly - close_curly)
            
            open_square = json_str.count('[')
            close_square = json_str.count(']')
            if open_square > close_square:
                json_str += "]" * (open_square - close_square)
            
            return json_str
        except Exception as e:
            logger.error(f"Error in advanced JSON fixing: {str(e)}")
            return json_str
    
    def _fix_json_line_by_line(self, json_str: str) -> str:
        """Fix JSON issues line by line, focusing on property name issues."""
        lines = json_str.split('\n')
        fixed_lines = []
        
        # Pattern to detect property without quotes
        property_pattern = re.compile(r'^\s*([a-zA-Z_][a-zA-Z0-9_\s]*)(\s*):')
        
        # Pattern to detect values without quotes
        value_pattern = re.compile(r':\s*([a-zA-Z][a-zA-Z0-9_\s]*)(\s*)(,|\}|$)')
        
        for line in lines:
            # Fix property names not in quotes
            if property_pattern.search(line):
                line = property_pattern.sub(r'    "\1"\2:', line)
            
            # Fix values not in quotes (if they're not special values like true, false, null)
            if value_pattern.search(line):
                match = value_pattern.search(line)
                value = match.group(1).strip().lower()
                if value not in ["true", "false", "null"] and not value.replace('.', '').isdigit():
                    line = value_pattern.sub(r': "\1"\2\3', line)
            
            fixed_lines.append(line)
        
        return '\n'.join(fixed_lines)
    
    def _extract_json_structure(self, text: str) -> Dict[str, Any]:
        """Extract JSON structure using regex patterns when all else fails."""
        # Initialize the basic structure
        result = {
            "candidate_name": "Unknown",
            "position": "Unknown",
            "strengths": [],
            "areas_for_improvement": [],
            "technical_evaluation": "",
            "cultural_fit": "",
            "recommendation": "",
            "next_steps": "",
            "overall_assessment": ""
        }
        
        # Extract candidate name and position
        name_match = re.search(r'"candidate_name"\s*:\s*"([^"]*)"', text)
        if name_match:
            result["candidate_name"] = name_match.group(1)
            
        position_match = re.search(r'"position"\s*:\s*"([^"]*)"', text)
        if position_match:
            result["position"] = position_match.group(1)
        
        # Extract strengths - handle both array of strings and array of objects formats
        strengths = []
        # Look for strengths section
        strengths_section = re.search(r'"strengths"\s*:\s*\[(.*?)\]', text, re.DOTALL)
        if strengths_section:
            strengths_content = strengths_section.group(1)
            
            # Try to extract strengths as strings first (proper format)
            string_strengths = re.findall(r'"([^"]*)"', strengths_content)
            if string_strengths:
                strengths.extend(string_strengths)
            
            # Then try to handle the object format with unquoted values 
            # Pattern like: {"Strength 1: X": Unquoted text here}
            object_strengths = re.findall(r'{\s*"([^"]*)"\s*:\s*([^{}]*?)\s*}', strengths_content)
            for key, value in object_strengths:
                # Combine key and value into a single string entry
                combined = f"{key}: {value.strip()}"
                strengths.append(combined)
                
            # If we still have no strengths, try a more aggressive approach
            if not strengths:
                # Look for anything that looks like "Strength X: " followed by text
                strength_items = re.findall(r'Strength\s+\d+[^"]*?:\s*([^",}]*)', strengths_content)
                strengths.extend([item.strip() for item in strength_items if item.strip()])
        
        result["strengths"] = strengths if strengths else ["Technical expertise", "Communication skills"]
            
        # Extract areas for improvement - with the same pattern handling
        improvements = []
        improvements_section = re.search(r'"areas_for_improvement"\s*:\s*\[(.*?)\]', text, re.DOTALL)
        if improvements_section:
            improvements_content = improvements_section.group(1)
            
            # Try string format
            string_improvements = re.findall(r'"([^"]*)"', improvements_content)
            if string_improvements:
                improvements.extend(string_improvements)
            
            # Try object format
            object_improvements = re.findall(r'{\s*"([^"]*)"\s*:\s*([^{}]*?)\s*}', improvements_content)
            for key, value in object_improvements:
                combined = f"{key}: {value.strip()}"
                improvements.append(combined)
                
            # Aggressive approach
            if not improvements:
                area_items = re.findall(r'Area\s+\d+[^"]*?:\s*([^",}]*)', improvements_content)
                improvements.extend([item.strip() for item in area_items if item.strip()])
        
        result["areas_for_improvement"] = improvements if improvements else ["Could provide more specific examples"]
        
        # Extract text fields - try both quoted and unquoted versions
        for field in ["technical_evaluation", "cultural_fit", "recommendation", "next_steps", "overall_assessment"]:
            # Try quoted version first
            quoted_pattern = f'"{field}"\\s*:\\s*"([^"]*)"'
            quoted_match = re.search(quoted_pattern, text, re.DOTALL)
            if quoted_match:
                result[field] = quoted_match.group(1)
            else:
                # Try unquoted version (common LLM mistake)
                unquoted_pattern = f'"{field}"\\s*:\\s*([^,"}}]+)'
                unquoted_match = re.search(unquoted_pattern, text, re.DOTALL)
                if unquoted_match:
                    result[field] = unquoted_match.group(1).strip()
                    
            # If using the multiline approach, clean up newlines and excessive whitespace
            if result[field]:
                result[field] = re.sub(r'\s+', ' ', result[field]).strip()
        
        return result
    
    def _create_fallback_summary(self, candidate_data: Dict[str, Any], job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Guaranteed fallback with meaningful content."""
        logger.warning("Using fallback summary")
        
        return {
            "candidate_name": candidate_data.get("name", "Candidate"),
            "position": job_data.get("title", "Position"),
            "strengths": [
                "Completed the interview process",
                "Engaged with the interviewer's questions",
                "Expressed interest in the position"
            ],
            "areas_for_improvement": [
                "More detailed responses would provide better insights",
                "Additional technical assessment recommended"
            ],
            "technical_evaluation": "A complete technical assessment is recommended to evaluate the candidate's skills more thoroughly.",
            "cultural_fit": "The candidate engaged with questions about the company. Additional discussion would help determine cultural alignment.",
            "recommendation": "Consider for a follow-up technical interview to better assess skills and fit.",
            "next_steps": "Schedule a technical assessment. Prepare focused questions on key skills required for the position.",
            "overall_assessment": f"{candidate_data.get('name', 'The candidate')} participated in the interview process. The system encountered technical limitations in generating a complete assessment, but the candidate's participation indicates interest in the position.",
            "response_scores": [
                {
                    "question_index": 0,
                    "score": 3,
                    "feedback": "Response demonstrated basic understanding of the question."
                },
                {
                    "question_index": 1,
                    "score": 3,
                    "feedback": "Candidate provided a relevant answer to the question."
                }
            ],
            "skill_ratings": [
                {"name": "Technical Knowledge", "score": 65},
                {"name": "Communication", "score": 70},
                {"name": "Problem Solving", "score": 60},
                {"name": "Domain Experience", "score": 55},
                {"name": "Team Collaboration", "score": 75}
            ]
        }

    def _initialize_llm_adapter(self, ollama_api_base=None, ollama_api_key=None):
        """Initialize the LLM adapter with the appropriate configurations."""
        logger.info("Initializing LLM adapter")
        
        if ollama_api_base is not None:
            os.environ["OLLAMA_API_BASE"] = ollama_api_base
        
        if ollama_api_key is not None:
            os.environ["OLLAMA_API_KEY"] = ollama_api_key
        
        self.llm_adapter = get_llm_adapter()
        logger.info(f"Using LLM adapter: {type(self.llm_adapter).__name__}")

    def generate_follow_up_question(self, current_question, response_text, job_data=None, company_data=None, candidate_data=None):
        """Generate a follow-up question with Llama3-optimized prompting"""
        logger.info("Generating follow-up question")
        
        # Create a carefully structured prompt for Llama3
        prompt = f"""<|system|>
You are an expert technical interviewer working for a leading technology company. You need to decide if a candidate's response requires a follow-up question.
</|system|>

<|user|>
I asked this interview question: "{current_question.get('question', '')}"

The candidate responded: "{response_text}"

For context, this is for a {job_data.get('title', 'position')} role.

Questions to consider:
1. Did the candidate fully answer the question?
2. Did they provide specific examples/details?
3. Is there a skill area that needs deeper probing?

If NO follow-up is needed, respond with exactly: "NO_FOLLOW_UP_NEEDED"

If a follow-up IS needed, respond with ONE brief, focused follow-up question (under 15 words) that references something specific from their response.
</|user|>
"""

        # Add timeout handling for demo reliability
        try:
            # Set a timer to ensure response within 5 seconds for demo
            import threading
            import queue
            
            result_queue = queue.Queue()
            
            def generate_with_timeout():
                try:
                    result = self._generate_with_llm(prompt, max_tokens=100)
                    result_queue.put(result)
                except Exception as e:
                    logger.error(f"Error in follow-up generation: {e}")
                    result_queue.put(None)
            
            thread = threading.Thread(target=generate_with_timeout)
            thread.daemon = True
            thread.start()
            
            # Wait for result with timeout appropriate for demo
            try:
                follow_up = result_queue.get(timeout=5)
                if follow_up is None or "NO_FOLLOW_UP_NEEDED" in follow_up:
                    return None
                    
                # Clean up the response - extract just the question
                import re
                question_match = re.search(r'([^.!?]*\?)', follow_up)
                if question_match:
                    return question_match.group(1).strip()
                
                # Remove any extra text
                follow_up = re.sub(r'(^|\n).*?(follow-up|followup).*?:', '', follow_up, flags=re.IGNORECASE)
                
                return follow_up.strip()
            except queue.Empty:
                logger.warning("Follow-up generation timed out")
                return "Could you elaborate more on that point?"
                
        except Exception as e:
            logger.error(f"Error generating follow-up: {e}")
            return "Could you share a specific example of that?"

    def generate_transition(self, contextual_prompt, current_response, current_question, next_question):
        """Generate a natural transition between questions based on conversation memory."""
        logger.info("Generating natural transition between questions")
        
        # Modify the prompt to explicitly instruct not to include meta-commentary
        prompt = contextual_prompt + "\n\nIMPORTANT: Return ONLY the transition text with no explanation or meta-commentary. Do not include phrases like 'This response acknowledges...' or explanations about what the transition does. Provide ONLY the text that should be shown to the candidate."
        
        try:
            transition = self._generate_with_llm(prompt, max_tokens=150)
            
            # Clean up the response
            transition = transition.strip()
            if transition.startswith('"') and transition.endswith('"'):
                transition = transition[1:-1].strip()
                
            # Remove any meta-commentary that might have been included despite instructions
            transition = self._remove_meta_commentary(transition)
            
            return transition
        except Exception as e:
            logger.error(f"Error generating transition: {e}")
            return next_question.get("transition", "Let's move on to the next question.")
            
    def _remove_meta_commentary(self, text: str) -> str:
        """Remove meta-commentary from transition text."""
        # Check for common meta-commentary patterns
        meta_patterns = [
            r"This (response|transition) acknowledges.*",
            r"This shows.*",
            r"This transition.*",
            r".*\bshows interest in\b.*",
            r".*\btransitioning into\b.*",
            r".*\bsmooth(ly)? transition(s|ing)?\b.*"
        ]
        
        # If any pattern is found in the text, extract only the first sentence
        import re
        for pattern in meta_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                # Extract only the first sentence which is likely the actual transition
                sentences = re.split(r'[.!?]\s+', text)
                if sentences:
                    return sentences[0].strip() + "."
                break
                
        return text

    def _generate_with_llm(self, prompt: str, max_tokens: int = 4000, temperature: float = 0.7) -> str:
        """Helper method to generate text using the LLM interface."""
        try:
            return self.llm.generate_text(prompt, max_tokens=max_tokens, temperature=temperature)
        except Exception as e:
            logger.error(f"Error generating text with LLM: {e}")
            raise 

    def _enhanced_extraction(self, text: str, candidate_data: Dict[str, Any], job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Enhanced extraction that works even with severely malformed responses."""
        import re
        
        # Initialize with fallback values
        result = {
            "candidate_name": candidate_data.get("name", "Candidate"),
            "position": job_data.get("title", "Position"),
            "strengths": [],
            "areas_for_improvement": [],
            "technical_evaluation": "",
            "cultural_fit": "",
            "recommendation": "",
            "next_steps": "",
            "overall_assessment": "",
            "response_scores": []
        }
        
        # Extract sections using markers that would be in the text regardless of JSON formatting
        sections = {
            "strengths": r"(?:key strengths|strengths).*?(?=areas for improvement|technical|cultural|recommendation|next steps|overall|$)",
            "areas_for_improvement": r"(?:areas for improvement|improvement areas|weaknesses).*?(?=technical|cultural|recommendation|next steps|overall|$)",
            "technical_evaluation": r"(?:technical.*?assessment|technical.*?evaluation).*?(?=cultural|recommendation|next steps|overall|$)",
            "cultural_fit": r"(?:cultural fit|company fit).*?(?=recommendation|next steps|overall|$)",
            "recommendation": r"(?:recommendation|hiring recommendation).*?(?=next steps|overall|$)",
            "next_steps": r"(?:next steps|follow-up steps).*?(?=overall|response quality|$)",
            "overall_assessment": r"(?:overall assessment|final assessment).*?(?=response quality|$)"
        }
        
        # Extract each section
        for field, pattern in sections.items():
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                content = match.group(0)
                
                # Process list fields differently
                if field in ["strengths", "areas_for_improvement"]:
                    # Look for numbered or bulleted items
                    items = re.findall(r'(?:^|\n)[\-*]?\s*\d*\.?\s*([^\-*\n\d\.][^\n]+)', content)
                    if items:
                        result[field] = [item.strip() for item in items if len(item.strip()) > 5]
                    else:
                        # Fallback - try to find sentences
                        sentences = re.findall(r'[^.!?]+[.!?]', content)
                        result[field] = [s.strip() for s in sentences if len(s.strip()) > 10][:3]
                else:
                    # For text fields, clean up and use the content
                    cleaned = re.sub(r'^.*?:', '', content, 1).strip()  # Remove field label
                    result[field] = cleaned
        
        # Extract response scores if available
        scores_section = re.search(r'(?:response quality|response scores).*?(?=$)', text, re.IGNORECASE | re.DOTALL)
        if scores_section:
            content = scores_section.group(0)
            # Look for question-score patterns like "Question X: Y/10" or similar
            score_items = re.findall(r'question[^:]*:[^:]*?(\d+)[/\s]*10', content, re.IGNORECASE)
            if score_items:
                for i, score in enumerate(score_items):
                    result["response_scores"].append({
                        "question": f"Question {i+1}",
                        "score": int(score),
                        "justification": "Extracted from unstructured text"
                    })
        
        # Fill missing fields with default values
        if not result["strengths"]:
            result["strengths"] = ["Strong communication skills", "Technical knowledge in relevant areas"]
            
        if not result["areas_for_improvement"]:
            result["areas_for_improvement"] = ["Could provide more specific examples"]
        
        # Validate and clean all fields
        for key, value in result.items():
            if isinstance(value, str):
                # Clean up text fields
                result[key] = re.sub(r'\s+', ' ', value).strip()
                
                # Set reasonable defaults for empty fields
                if not result[key]:
                    if key == "technical_evaluation":
                        result[key] = "Demonstrated adequate technical skills for the position."
                    elif key == "cultural_fit":
                        result[key] = "Appears to align with company values."
                    elif key == "recommendation":
                        result[key] = "Consider for next interview round."
                    elif key == "next_steps":
                        result[key] = "Complete technical assessment."
                    elif key == "overall_assessment":
                        result[key] = "Candidate shows potential for the role."
        
        return result 